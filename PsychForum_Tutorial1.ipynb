{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2_CCN_Training_SOLUTION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsscholte/psyForum/blob/master/PsychForum_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDbCH-zG4IvJ",
        "colab_type": "text"
      },
      "source": [
        "#Psyforum Tutorial 1: Training a deep neural network (v1.4)\n",
        "Lynn Sörensen & H.Steven Scholte\n",
        "\n",
        "### Welcome! \n",
        "In this tutorial you will learn how to train a convolutional deep neural network (DCNN) on a simple object recognition task.\n",
        "\n",
        "You will \n",
        "* build a Resnet model and look at its architecture <br/>\n",
        "* train it on CIFAR10 <br/>\n",
        "* ...with different learning schedules and observe the effects <br/>\n",
        "\n",
        "next, you will\n",
        "* design your own model and training scheme to get an optimal classification on a task. \n",
        "\n",
        "This tutorial is also going to get you acquainted with a dataset and a model architecture that will be used in a later tutorial. \n",
        "\n",
        "#### Some extra information:\n",
        "There are different types of exercises throughout this tutorial, including:\n",
        "\n",
        "- <font color='darkorange'><b>ToDos</b></font> : short programming exercises \n",
        "- <font color='cornflowerblue'><b>ToThinks</b></font>: questions about the (preceding) text/material\n",
        "\n",
        "Sometimes, you also encounter <font color='limegreen'><b>Tips and Tricks</b></font>, which may contain advice, more information on a specific topic, or links to relevant websites or material.\n",
        "\n",
        "#### GPU support\n",
        "Don't forget to change your runtime type to GPU accelerator if you want to train these model. Click on `Runtime` above this notebook in the Colab environment and then on `Change Runtime Type`. Set Hardware accelerator to `GPU`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikPkQ_8eKRf7",
        "colab_type": "text"
      },
      "source": [
        "### Let's start!\n",
        "\n",
        "With importing the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOtWQVH34e9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBO77imOswz1",
        "colab_type": "text"
      },
      "source": [
        "And connect our google drive!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPOCjy-cs1xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz5hRTSFjE7Z",
        "colab_type": "text"
      },
      "source": [
        "And lets create a directory in your gdrive to store the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4WDh6LUjFRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir \"/gdrive/My Drive/PsyForum\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GS2we05FD-m",
        "colab_type": "text"
      },
      "source": [
        "And define some useful functions for later!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgB0UCVlE7XW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_pickle(obj, name):\n",
        "    try:\n",
        "        filename = open(name + \".pickle\",\"wb\")\n",
        "        pickle.dump(obj, filename)\n",
        "        filename.close()\n",
        "        return(True)\n",
        "    except:\n",
        "        return(False)\n",
        "\n",
        "\n",
        "def load_pickle(filename):\n",
        "    return pickle.load(open(filename, \"rb\"))\n",
        "  \n",
        "def plot_training(history, title=None):\n",
        "  epochs = len(history.history['loss'])\n",
        "  if 'lr' in history.history:\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20,5))\n",
        "    ax = ax.flatten()\n",
        "    ax[0].plot(np.arange(epochs), np.transpose([history.history['loss'], history.history['val_loss']]))\n",
        "    ax[0].set_xlabel('epochs')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].set_ylim([0,3])\n",
        "    ax[0].legend(['training', 'validation'])\n",
        "\n",
        "    ax[1].plot(np.arange(epochs), np.transpose([history.history['acc'], history.history['val_acc']]))\n",
        "    ax[1].set_xlabel('epochs')\n",
        "    ax[1].set_ylabel('accuracy')\n",
        "    ax[1].set_ylim([0,1])\n",
        "    ax[1].legend(['training', 'validation'])\n",
        "    \n",
        "    ax[2].plot(np.arange(epochs), history.history['lr'])\n",
        "    ax[2].set_xlabel('epochs')\n",
        "    ax[2].set_ylabel('learning rate')\n",
        "    #ax[2].set_ylim([0,1])\n",
        "  else:\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "    ax = ax.flatten()\n",
        "    ax[0].plot(np.arange(epochs), np.transpose([history.history['loss'], history.history['val_loss']]))\n",
        "    ax[0].set_xlabel('epochs')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].set_ylim([0,3])\n",
        "\n",
        "    ax[1].plot(np.arange(epochs), np.transpose([history.history['acc'], history.history['val_acc']]))\n",
        "    ax[1].set_xlabel('epochs')\n",
        "    ax[1].set_ylabel('accuracy')\n",
        "    ax[1].set_ylim([0,1])\n",
        "    fig.legend(['training', 'validation'])\n",
        "  if title:\n",
        "    fig.suptitle(title)\n",
        "  fig.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43fLIj0Io043",
        "colab_type": "text"
      },
      "source": [
        "**bold text**## Part 1: Train a network on a simple object recognition task with different training schemes\n",
        "In this part of the tutorial, you will be training a DCNN to solve object categorization on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). We will use this dataset because it is relatively quick to train and still allows you to observe interesting behavior. The specific version of ResNet is based on [ResNetCifar10](http://www.pabloruizruiz10.com/resources/CNNs/ResNet-on-CIFAR10.pdf).\n",
        "\n",
        "The logic of this part of the tutorial will be that you observe that effects of different choices (e.g. learning rate, data augmentation) that you make when you decide to train a DCNN from scratch. Training a DCNN can be tricky and the interaction between different decisions difficult to anticipate. \n",
        "\n",
        "Our goal for the first part is to replicate the training from the Resnet paper, one of the most successful architectures that is still widely used to day, despite its relatively old age for the deep learning community. To do this, we will be adding component by component to our training and obeserve in how it helps. \n",
        "\n",
        "So let's get started!\n",
        "\n",
        "For all of the following experiments, we need:\n",
        "* A stimulus set - CIFAR10\n",
        "* A ResNet architecture\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1kPWpL-EZV",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Stimulus set: CIFAR10\n",
        "The CIFAR10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.\n",
        "\n",
        "This dataset is a standard benchmark in the Computer vision community to benchmark new architecture and training regimes.\n",
        "\n",
        "<font color='darkorange'><b>ToDo:</b></font> Fill in the missing parts in the code below by replacing the 'FILL'.\n",
        "Check out the objects. Plot a couple of examples for every category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fqRiZRuveso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# How many classes are there?\n",
        "num_classes = 10\n",
        "\n",
        "# Plots some examples here.\n",
        "num_examples = 5\n",
        "fig, ax = plt.subplots(num_examples, num_classes, figsize=(20,10))\n",
        "for c in range(num_classes):\n",
        "  class_idx = np.where(y_train == c)[0] # for a given class\n",
        "  for e in range(num_examples): #pick some random examples from the training set\n",
        "    # ax[e,c].imshow(FILL) \n",
        "    ax[e,c].imshow(x_train[class_idx[np.random.randint(len(class_idx))],:,:,:]) \n",
        "    ax[e,c].axis('off')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_37UZjxozgb",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Data preparation\n",
        "Humans and networks need different inputs and labels. Comparable to the range of light intensities that we are able to perceive, we need to transfer the images into a space where the networks can easily compute on them. This means, we will have to convert our images to have values between 0 and 1. \n",
        "\n",
        "<font color='cornflowerblue'><b>ToThink:</b></font> Why do you think the network can deal better with inputs between 0-1?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWbID3V6W3Ut",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: Inputs in a bounded range allow the network to learn and find weights that are not too extreme. Since we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model.\n",
        "\n",
        "We'd like in this process for each feature to have a similar range so that our gradients don't go out of control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlXgBC83W79h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<font color='darkorange'><b>ToDo:</b></font> Fill in the missing parts in the code below by replacing the 'FILL'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgimuKrpvMQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEbtKrQV9nb",
        "colab_type": "text"
      },
      "source": [
        "While we have an easy time understanding the label 'bird', we need to express categories as numbers for the network. The next step will be to also express the labels of the target category as a 1-hot-vector. This just means that you have a vector with zeros corresponding to all labels and you put a 1 for the right target category. Fortunately, Keras has a nice set of 'utils' to help us with that. For example keras.utils.to_categorical converts a class vector of integers to a binary class matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32-d2bDKWcxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOFtA-kTR6F",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Validation set\n",
        "\n",
        "As a last step, we'll set aside some of our training data as a validation dataset. This allows us to estimate how well our current model performs without touching the test set. \n",
        "\n",
        "<font color='cornflowerblue'><b>ToThink:</b></font> Do you recall why it is useful to have a validation set?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCwURA1GXDyE",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: A validation set gives you an estimate of how well your current solution of weights will generalize to the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQcAGnSlfZiL",
        "colab_type": "code",
        "outputId": "8fefc3ba-f437-4fd3-d9d0-ec7fbdffdb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# We take here 10% of our training set as a validation set. \n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=3) # 30k train, 10k val\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "print('x_val shape:', x_val.shape)\n",
        "print(x_val.shape[0], 'val samples')\n",
        "print('y_val shape:', y_val.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (37500, 32, 32, 3)\n",
            "37500 train samples\n",
            "y_train shape: (37500, 10)\n",
            "x_val shape: (12500, 32, 32, 3)\n",
            "12500 val samples\n",
            "y_val shape: (12500, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHCdacIw6_i",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<font color='limegreen'><b>Tips and Tricks</b></font>: Here we use a function from sklearn, this is one way to do it. Keras also allows you to make a validation set on the fly with an argument validationSplit in the [ImageDataGenerator class](https://keras.io/preprocessing/image/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ZrFhPCi1wF",
        "colab_type": "text"
      },
      "source": [
        "Notice that we also still have a test set that we obtained when we loaded cifar10. Below we inspect its dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcB7V8Qsi2Ng",
        "colab_type": "code",
        "outputId": "afb400c8-d8f3-4b82-e5d7-3de260aa4080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('x_test shape:', x_test.shape)\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_test shape: (10000, 32, 32, 3)\n",
            "10000 test samples\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF3yfVvvjRqS",
        "colab_type": "text"
      },
      "source": [
        "<font color='cornflowerblue'><b>ToThink:</b></font> Why do you think we have 3 datasets, train, validate and test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSKk4wW-Blo9",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Resnet architecture\n",
        "\n",
        "Now let's move on to the Resnet architecture. Since the images of CIFAR10 are only 32x32x3, the network to classify them cannot be too deep (remember the effect of convolution and pooling operations). For that reason, the authors of Resnet made a mini-version tailored to CIFAR10. The most important differences to the Resnet-version used on Imagenet (224x224x3 images) is the amount of blocks and the depth of the filters. Also, to somewhat reduce the performance we have made the network slightly smaller. \n",
        "\n",
        "<font color='darkorange'><b>ToDo:</b></font> Go over the definitions below and make sure to understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SswcBKvjBGqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Below you can see the definition of a resnet block. \n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    # This is how you defince a 2D convolutional layer. \n",
        "    # At this point this is just a pre-defined function and not a layer yet!\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer= keras.regularizers.l2(1e-4))\n",
        "    # This is where we start to chain the layers together.\n",
        "    x = inputs # any block will start with the inputs given to this function.\n",
        "    if conv_first: # This if-else statement refers to that there are two-ways of constructing the Resnet-block.\n",
        "        x = conv(x) # Append the conv function from above to the input. \n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x) # Here we add the Batchnormalization in the same way. \n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x) # And similarly for the Activation function.\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "  \n",
        "  # These operations will be repeated many times and as result \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ1chexLFGwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and this is how the model is put together.\n",
        "  \n",
        "def resnet_v1(input_shape, depth=8, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "    Adapted from https://keras.io/examples/cifar10_resnet/\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet10 0.14M\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor (dim1,dim2,channels)\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    # if (depth - 2) % 6 != 0:\n",
        "    #    raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs) \n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            # This defines the first branch in which all the weight layer are. \n",
        "            y = resnet_layer(inputs=x, \n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y, \n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y]) # Here the first branch is added to the residual.\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoPY3mU-7U8L",
        "colab_type": "text"
      },
      "source": [
        "<font color='darkorange'><b>ToDo:</b></font> Build the model with the functions above (by using the default settings!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53LDH0lE7RK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = resnet_v1(input_shape=input_shape)\n",
        "\n",
        "# ... and show the architecture\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQPtUnh2EuDi",
        "colab_type": "text"
      },
      "source": [
        "Voilà, your model!\n",
        "So what is going on inside of a Resnet-Model?\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png) \n",
        "\n",
        "Above the same operations are shown as a graphic. x refers to the current end of the model, that can be any layer. A weight later would a Conv2D layer. relu refers to the activation function. In our case, this is also integrated in the Conv2D layer (see the activation argument in the resnet layer function). \n",
        "\n",
        "<font color='cornflowerblue'><b>ToThink:</b></font>  Can you see how this schematic fits with the model.summary output? \n",
        "How many of these blocks are there in total? Follow the Add operation!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Rzhun7Y2vd",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: There are 3 blocks in total."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33EadtfRY7Fm",
        "colab_type": "text"
      },
      "source": [
        "There are range of different resnet types with varying depths! The depths in the name is determined by the amount of Convolutional and Dense layers.\n",
        "\n",
        "<font color='cornflowerblue'><b>ToThink:</b></font> In the current case, how many convolutional and dense layers are there respectively?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiHVndvY7UH",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: There are 9 convolutional and 1 dense layer, making it a 10 layer network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ljLZF_8jIB",
        "colab_type": "text"
      },
      "source": [
        "<font color='limegreen'><b>Tips and Tricks</b></font>: There is a range of models available provided by Keras with pretrained weights on ImageNet that are easy to use such as VGG or Inception. For more info see [here](https://keras.io/applications/#available-models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qU2GpeiKHKU",
        "colab_type": "text"
      },
      "source": [
        "### 1.5 Training options\n",
        "\n",
        "Our model currently has random weights, that means, it has not been trained to do any task. Let's see what that means. For example if we have it classify the CIFAR10 training set.\n",
        "\n",
        "#### 1.5.0 model.compile\n",
        "Before a freshly assembled model can be trained or tested, it first needs to be compiled. That means we add our loss function (e.g. categorical_crossentropy), a training optimizer (remember stochastic gradient descent!) and a metric to compare between the model answer and the true answer (accuracy).\n",
        "\n",
        "\n",
        "<font color='darkorange'><b>ToDo:</b></font> Check out https://keras.io/optimizers/ for an overview of all available optimizers and their defaults. Set up the compile function with categorical crossentropy, stochastic gradient descent and accuracy as a metric!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZefbvyGBDV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = keras.optimizers.SGD() # select optimizer\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['acc'])  # select way of calculating loss and expressing accuracy\n",
        "scores = model.evaluate(x_train, y_train, verbose=1) # evaluate model\n",
        "\n",
        "print('Training loss:', scores[0])\n",
        "print('Training accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N4g0xdnO6WA",
        "colab_type": "text"
      },
      "source": [
        "<font color='cornflowerblue'><b>ToThink:</b></font> What does the training loss mean? Is the training accuracy like you would expect and if so, why? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R1oOHGHnBql5"
      },
      "source": [
        "#### 1.5.1 Training with standard gradient descent and a large batch size\n",
        "Now let's start with teaching the model the task of distinguishing the 10 different object categories in CIFAR10. \n",
        "\n",
        "In the lecture, we have seen that a big part of successful training is choosing the right batch size, gradient update & learning rate at the right moment in time. Below, we are going to train our model in different ways and change this part in our training. \n",
        "\n",
        "As a result of how we set up our model above (model.compile ...), we accepted the default parameters of the SGD optimizer. The gradient updates will follow the steepest gradient throughout our entire training. \n",
        "\n",
        "<font color='cornflowerblue'><b>ToThink:</b></font> Do you think this is a good strategy?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ScXzG44VBqmA"
      },
      "source": [
        "ANSWER: In general, there is nothing wrong about accepting the default values. Yet the basic concept of gradient descent is great but there might be some smarter ways to selet these values. Figuring out which these are, in such a way that the model is not overfitted is a central skill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q0pTAoXDBuL7"
      },
      "source": [
        "<font color='darkorange'><b>ToDo:</b></font> Can you set up the training below with the model.fit() function? Remember, we set aside a validation dataset, to evaluate how well our model will generalize to the test set! Choose a batch-size of 5000 and run it for 30 epochs!\n",
        "\n",
        "<font color='limegreen'><b>Tips and Tricks</b></font>: In the snippet of code there is already a modelcheckpoint set up for you. This will safe the currently best-performing model automatically. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cYtbK7VXBzuZ",
        "colab": {}
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 5000\n",
        "\n",
        "# Training of the model\n",
        "history_151 = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val), shuffle=True)\n",
        "\n",
        "# Save the outcomes\n",
        "model.save_weights('/gdrive/My Drive/PsyForum/weights_151.h5')\n",
        "save_pickle(history_151, '/gdrive/My Drive/PsyForum/history_151')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-gfPV2y-k_AR"
      },
      "source": [
        "The history object contains an overview of the training. With that you can inspect how the training went. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "32f1LYtnk_AV",
        "colab": {}
      },
      "source": [
        "plot_training(history_151, title='1.5.1 Training with a static learning rate, large batch size')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY1PxF-i6kNx",
        "colab_type": "text"
      },
      "source": [
        "#### 1.5.2 Training with a fixed learning rate schedule\n",
        "\n",
        "As we discussed in the lecture, learning rate is one of the crucial hyperparameters, which is subject to optimization. If you in turn would like to replicate another paper, they sometimes mention a learning rate schedule that is optimized for this specific architecture and dataset. \n",
        "\n",
        "<font color='darkorange'><b>ToDo:</b></font> Can you come up with your own learning rate schedule below? Again, you can keep all the other parameters the same. Once, you are done, look at the provided training outcomes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXHmRxiF-uWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 36\n",
        "\n",
        "model_152 = resnet_v1(input_shape=input_shape)\n",
        "\n",
        "# We implement a modern optimizer (nesterov)\n",
        "sgd = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True) \n",
        "  \n",
        "def step_decay(epoch): \n",
        "  if epoch  < 10:\n",
        "    lrate=0.1\n",
        "  else:\n",
        "    lrate = 0.05\n",
        "  return lrate\n",
        "\n",
        "# and we hand the function to the training with acallback.\n",
        "lr_reducer = keras.callbacks.LearningRateScheduler(step_decay)\n",
        "\n",
        "model_152.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "history_152 = model_152.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_val, y_val),shuffle=True, callbacks = [lr_reducer])\n",
        "\n",
        "model_152.save_weights('/gdrive/My Drive/PsyForum/weights_152.h5')\n",
        "save_pickle(history_152, '/gdrive/My Drive/PsyForum/history_152')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8sYgwxIndg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_training(history_152, title='1.5.2 Training with a fixed learning rate schedule, small batch size and modern optimizer')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwokFvBT9k9g",
        "colab_type": "text"
      },
      "source": [
        "<font color='darkorange'><b>ToDo:</b></font> Plot all our training histories in one plot!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZe5zLr583LA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_pickle('/gdrive/My Drive/PsyForum/history_151.pickle')\n",
        "load_pickle('/gdrive/My Drive/PsyForum/history_152.pickle')\n",
        "\n",
        "plot_training(history_151, title='1.5.1 Training with large batch size and a fixed learning rate schedule')\n",
        "plot_training(history_152, title='1.5.2 Training with a fixed learning rate schedule, small batch size and modern optimizer')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DCpuAbhB5IkP"
      },
      "source": [
        "<font color='cornflowerblue'><b>ToThink:</b></font> Which one worked the best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvqMmncd8dF",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: Based on the comparison, the fixed training schedule seemed to have worked best. This is likely due to the fact that this schedule had a late phase with a very small learing rate, which allowed for very fine adjustments and that this was lacking in the other trainings. Also, this fixed schedule was based on the data-set making it a hyper parameter that has been optimized. "
      ]
    }
  ]
}